# Training #
lr: 0.0001
lr_backbone: 1.0e-05
batch_size: 1
weight_decay: 0.0001
epochs: 18
lr_drop: 12
clip_max_norm: 0.1
eval_interval: 1

# Model paramters #
pretrained_weights:
backbone: resnet101
return_interm_backbone_layers: false
dilation: false
position_embedding: sine
num_classes: 2

# Transformer #
num_layers: 6
forward_expansion_ratio : 4
embed_size: 2048 # 384
drop_path: 0.1
forward_drop_p: 0.1
nheads: 8
norm_output:
num_frames: 40
num_queries: 90
pre_norm: false

# Loss #
aux_loss: true
# Matcher
set_cost_class: 1
set_cost_bbox: 5
set_cost_giou: 2
# Loss coefficients
#mask_loss_coef: 1
#dice_loss_coef: 1
#bbox_loss_coef: 5
#giou_loss_coef: 2
#eos_coef: 0.1

# Dataset parameters
dataset_name: picai2022 # proles2021_debug   picai2022
dataset_path: /mnt/DATA2/Sagi/Data/PICAI/processed_data/processed_data_resgist_normalized/
modalities: all # 'all' for all available modalities. For specific modalities in a list of the desired ones (example: [])
remove_difficult: false
output_dir: results
device: cuda
seed: 42
resume: ''
start_epoch: 0
eval: false
num_workers: 4

# Distributed training parameters
world_size: 1
dist_url: env://
